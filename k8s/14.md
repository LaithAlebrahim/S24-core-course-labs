# Kubernetes Monitoring and Init Containers

## Kubernetes Cluster Monitoring with Prometheus

### Description of Components 
- **Prometheus**: gathers and saves time-series data about the activities of your cluster.

- **Prometheus Operator**: simplifies the configuration and administration of Prometheus and its parts in Kubernetes. Consider it your assistance in maintaining Prometheus's configuration and updates.

- **Grafana**: A well-liked open-source visualization tool for making complex dashboards is Grafana. Grafana is set up on the Kube Prometheus Stack with pre-made dashboards specifically designed for Kubernetes cluster monitoring. These dashboards include information on resource utilization, application performance, and cluster health.


- **Alertmanager**: Prometheus alerts are managed with Alertmanager. Users can specify alerting rules and choose which channels, such as email, Slack, PagerDuty, etc., notifications should be sent to. This part makes sure that important issues are notified in a timely manner and makes incident response easier.

- **Node Exporter**: collects comprehensive data regarding the hardware and operating system conditions of every node (such as memory or CPU usage). It resembles giving each server a physical examination.


- **Kube State Metrics**: A service called Kube State Metrics gathers metrics on various Kubernetes objects, including deployments, pods, nodes, and namespaces. It gives operators information about the condition and health of Kubernetes resources, which aids in their understanding of the cluster's general health.

- **Prometheus Blackbox Exporter**: determines how well external responses to your network services are working. It may check, for instance, whether your website is up and how quickly it loads from various places.


- **Prometheus Adapter**: Kubernetes installations can automatically scale according to custom metrics that Prometheus gathers thanks to the Prometheus Adapter. By integrating Prometheus metrics with the Kubernetes custom metrics API, it makes it possible for autoscaling of horizontal pods according to application-specific metrics.

### Helm Chart Installation 
```shell
laith@DESKTOP-PPRJIN9:/mnt/d/S24-core-course-labs/k8s$ kubectl get po,sts,svc,pvc,cm
NAME                                                                  READY   STATUS    RESTARTS   AGE
pod/alertmanager-prometheus-my-helm-app-pyt-alertmanager-0            2/2     Running   0          19m
pod/app-python-app-python-0                                           1/1     Running   0          115s
pod/app-python-app-python-1                                           1/1     Running   0          115s
pod/prometheus-my-helm-app-pyt-operator-8588d97b46-5gzqr              1/1     Running   0          7m21s
pod/prometheus-my-helm-app-python-grafana-6454d9f9f-9sqf2             3/3     Running   0          7m21s
pod/prometheus-my-helm-app-python-kube-state-metrics-787f48966f4xl    1/1     Running   0          7m21s
pod/prometheus-my-helm-app-python-prometheus-node-exporter-mzw9k      1/1     Running   0          7m21s
pod/prometheus-prometheus-my-helm-app-pyt-prometheus-0                2/2     Running   0          6m59s

NAME                                                                    READY   AGE
statefulset.apps/alertmanager-prometheus-my-helm-app-pyt-alertmanager   1/1     6m59s
statefulset.apps/app-python-app-python                                  2/2     115s
statefulset.apps/prometheus-prometheus-my-helm-app-pyt-prometheus       1/1     6m59s

NAME                                                             TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE
service/alertmanager-operated                                    ClusterIP      None             <none>        9093/TCP,9094/TCP,9094/UDP   19m
service/app-python-app-python                                    LoadBalancer   10.108.234.64    <pending>     5000:30951/TCP               115s
service/kubernetes                                               ClusterIP      10.96.0.1        <none>        443/TCP                      21m
service/prometheus-my-helm-app-pyt-alertmanager                  ClusterIP      10.107.137.19    <none>        9093/TCP,8080/TCP            19m
service/prometheus-my-helm-app-pyt-operator                      ClusterIP      10.97.228.43     <none>        443/TCP                      19m
service/prometheus-my-helm-app-pyt-prometheus                    ClusterIP      10.108.82.205    <none>        9090/TCP,8080/TCP            19m
service/prometheus-my-helm-app-python-grafana                    ClusterIP      10.105.178.30    <none>        80/TCP                       19m
service/prometheus-my-helm-app-python-kube-state-metrics         ClusterIP      10.111.96.29     <none>        8080/TCP                     19m
service/prometheus-my-helm-app-python-prometheus-node-exporter   ClusterIP      10.101.99.125    <none>        9100/TCP                     19m
service/prometheus-operated                                      ClusterIP      None             <none>        9090/TCP                     19m

NAME                                                        STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
persistentvolumeclaim/Data-app-python-helm-app-python-0     Bound    pvc-11feca6b-be84-4789-86f7-c81f5ec96ad5   2Gi        RWO            standard       115s
persistentvolumeclaim/Data-app-python-helm-app-python-1     Bound    pvc-072999ba-a7e7-4944-a4aa-343e1ed65d4b   2Gi        RWO            standard       115s

NAME                                                                     DATA   AGE
configmap/kube-root-ca.crt                                               1      115s
configmap/prometheus-my-helm-app-pyt-alertmanager-overview               1      7m22s
configmap/prometheus-my-helm-app-pyt-apiserver                           1      7m22s
configmap/prometheus-my-helm-app-pyt-cluster-total                       1      7m22s
configmap/prometheus-my-helm-app-pyt-controller-manager                  1      7m22s
configmap/prometheus-my-helm-app-pyt-etcd                                1      7m22s
configmap/prometheus-my-helm-app-pyt-grafana-datasource                  1      7m22s
configmap/prometheus-my-helm-app-pyt-grafana-overview                    1      7m22s
configmap/prometheus-my-helm-app-pyt-k8s-coredns                         1      7m22s
configmap/prometheus-my-helm-app-pyt-k8s-resources-cluster               1      7m22s
configmap/prometheus-my-helm-app-pyt-k8s-resources-multicluster          1      7m22s
configmap/prometheus-my-helm-app-pyt-k8s-resources-namespace             1      7m22s
configmap/prometheus-my-helm-app-pyt-k8s-resources-node                  1      7m22s
configmap/prometheus-my-helm-app-pyt-k8s-resources-pod                   1      7m22s
configmap/prometheus-my-helm-app-pyt-k8s-resources-workload              1      7m22s
configmap/prometheus-my-helm-app-pyt-k8s-resources-workloads-namespace   1      7m22s
configmap/prometheus-my-helm-app-pyt-kubelet                             1      7m22s
configmap/prometheus-my-helm-app-pyt-namespace-by-pod                    1      7m22s
configmap/prometheus-my-helm-app-pyt-namespace-by-workload               1      7m22s
configmap/prometheus-my-helm-app-pyt-node-cluster-rsrc-use               1      7m22s
configmap/prometheus-my-helm-app-pyt-node-rsrc-use                       1      7m22s
configmap/prometheus-my-helm-app-pyt-nodes                               1      7m22s
configmap/prometheus-my-helm-app-pyt-nodes-darwin                        1      7m22s
configmap/prometheus-my-helm-app-pyt-persistentvolumesusage              1      7m22s
configmap/prometheus-my-helm-app-pyt-pod-total                           1      7m22s
configmap/prometheus-my-helm-app-pyt-prometheus                          1      7m22s
configmap/prometheus-my-helm-app-pyt-proxy                               1      7m22s
configmap/prometheus-my-helm-app-pyt-scheduler                           1      7m22s
configmap/prometheus-my-helm-app-pyt-workload-total                      1      7m22s
configmap/prometheus-my-helm-app-python-grafana                          1      7m22s
configmap/prometheus-my-helm-app-python-grafana-config-dashboards        1      7m22s
configmap/prometheus-prometheus-my-helm-app-pyt-prometheus-rulefiles-0   35     7m22s
configmap/py-configmap                                                   1      115s
```

## Kubernetes Cluster Summary

### Overview
A snapshot of the different resources maintained in the Kubernetes environment, especially those associated with a Prometheus-based monitoring system, can be obtained from the output of the command `kubectl get po,sts,svc,pvc,cm`.



### Pods
- The operation of **Alertmanager, Prometheus, Grafana, and associated services** shows that the infrastructure for monitoring is in good condition.
- **Application Pods** Two pods for my Python application.

### StatefulSets
- The **Alertmanager** and **Prometheus** deployments are stable, indicating that coordinating was successful and there are no outstanding updates. The desired state matches the current state.

### Services
**ClusterIP** and **LoadBalancer** services work together to improve service accessibility and communication both inside and maybe outside of the cluster. The majority of services concentrate on internal cluster communication and don't expose an external IP.

### PersistentVolumeClaims
- Application data is stored on persistent storage, and all volumes are presently bounded and functional.

### ConfigMaps
 - There are a lot of **ConfigMaps** that offer configurations for different Prometheus monitoring setups and cluster component parts. This covers alert rules, dashboards, metrics scraping, and service discovery settings.

### Grafana Dashboards
#### CPU and Memory consumption
```
CPU consumption: 0,001513%
Memory consumption: 40.4 MB
```

#### Pods with higher and lower CPU usage in the default namespace
```
Lowest usage: alertmanager-prometheus-my-helm-app-pyt-alertmanager-0
Hihest usage: prometheus-prometheus-my-helm-app-pyt-prometheus-0
```

#### Node memory usage in percentage and megabytes
```
71.2% (2.39 GiB)
```

#### Number of pods and containers managed by the Kubelet service
```
21 pods
53 containers
```

#### Network usage of Pods in the default namespace 
```
36.4 kB/s for download, 21.6 kB/s for upload
```
#### Number of active alerts
```
9 active alerts
```

## Init Containers

```shell
laith@DESKTOP-PPRJIN9:/mnt/d/S24-core-course-labs/k8s$kubectl exec statefulset.apps/app-python-app-python -- head -n 5 /work-dir/index.html
Defaulted container "app-python" out of: app-python, get-page (init)
<html><head></head><body><header>
<title>http://info.cern.ch</title>
</header>
<h1>http://info.cern.ch - home of the first website</h1>
```